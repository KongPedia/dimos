services:
  # Simulation profile
  dimos_simulation:
    build:
      context: ../..
      dockerfile: docker/navigation/Dockerfile
      network: host
      args:
        ROS_DISTRO: ${ROS_DISTRO:-humble}
    image: dimos_autonomy_stack:${ROS_DISTRO:-humble}
    container_name: dimos_simulation_container
    profiles: ["", "simulation"]  # Active by default (empty profile) AND with --profile simulation

    # Shared memory size for ROS 2 FastDDS
    shm_size: '8gb'

    # Enable interactive terminal
    stdin_open: true
    tty: true

    # Network configuration - required for ROS communication
    network_mode: host

    # Allow `ip link set ...` (needed by DimOS LCM autoconf) without requiring sudo
    cap_add:
      - NET_ADMIN

    # Use nvidia runtime for GPU acceleration (falls back to runc if not available)
    runtime: ${DOCKER_RUNTIME:-nvidia}

    # Environment variables for display and ROS
    environment:
      - DISPLAY=${DISPLAY}
      - QT_X11_NO_MITSHM=1
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=${NVIDIA_DRIVER_CAPABILITIES:-all}
      - ROS_DOMAIN_ID=${ROS_DOMAIN_ID:-42}
      - ROBOT_CONFIG_PATH=${ROBOT_CONFIG_PATH:-mechanum_drive}
      - ROBOT_IP=${ROBOT_IP:-}
      - HARDWARE_MODE=false
      # DDS Configuration (FastDDS)
      - RMW_IMPLEMENTATION=rmw_fastrtps_cpp
      - FASTRTPS_DEFAULT_PROFILES_FILE=/ros2_ws/config/fastdds.xml

    # Volume mounts
    volumes:
      # X11 socket for GUI
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
      - ${HOME}/.Xauthority:/root/.Xauthority:rw

      # Mount Unity environment models (if available)
      - ./unity_models:/ros2_ws/src/ros-navigation-autonomy-stack/src/base_autonomy/vehicle_simulator/mesh/unity:rw

      # Mount entire dimos directory for live development
      - ../..:/workspace/dimos:rw

      # Mount bagfiles directory
      - ./bagfiles:/ros2_ws/bagfiles:rw

      # Mount config files for easy editing
      - ./config:/ros2_ws/config:rw

    # Device access (for joystick controllers)
    devices:
      - /dev/input:/dev/input
      - /dev/dri:/dev/dri

    # Working directory
    working_dir: /workspace/dimos

    # Command to run both ROS and DimOS
    command: /usr/local/bin/run_both.sh

  # Hardware profile - for real robot
  dimos_hardware:
    build:
      context: ../..
      dockerfile: docker/navigation/Dockerfile
      network: host
      args:
        ROS_DISTRO: ${ROS_DISTRO:-humble}
    image: dimos_autonomy_stack:${ROS_DISTRO:-humble}
    container_name: dimos_hardware_container
    profiles: ["hardware"]

    # Shared memory size for ROS 2 FastDDS
    shm_size: '8gb'

    # Load environment from .env file
    env_file:
      - .env

    # Enable interactive terminal
    stdin_open: true
    tty: true

    # Network configuration - MUST be host for hardware access
    network_mode: host

    # Privileged mode REQUIRED for hardware access
    privileged: true

    # Override runtime for GPU support
    runtime: ${DOCKER_RUNTIME:-runc}

    # Add host groups for device access (input for joystick, dialout for serial)
    group_add:
      - ${INPUT_GID:-995}
      - ${DIALOUT_GID:-20}

    # Hardware environment variables
    environment:
      - DISPLAY=${DISPLAY:-:0}
      - QT_X11_NO_MITSHM=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - ROS_DOMAIN_ID=${ROS_DOMAIN_ID:-42}
      - ROBOT_CONFIG_PATH=${ROBOT_CONFIG_PATH:-mechanum_drive}
      - ROBOT_IP=${ROBOT_IP:-}
      - HARDWARE_MODE=true
      # DDS Configuration (FastDDS)
      - RMW_IMPLEMENTATION=rmw_fastrtps_cpp
      - FASTRTPS_DEFAULT_PROFILES_FILE=/ros2_ws/config/fastdds.xml
      # Mid-360 Lidar configuration
      - LIDAR_INTERFACE=${LIDAR_INTERFACE:-}
      - LIDAR_COMPUTER_IP=${LIDAR_COMPUTER_IP:-192.168.1.5}
      - LIDAR_GATEWAY=${LIDAR_GATEWAY:-192.168.1.1}
      - LIDAR_IP=${LIDAR_IP:-192.168.1.116}
      # Motor controller
      - MOTOR_SERIAL_DEVICE=${MOTOR_SERIAL_DEVICE:-/dev/ttyACM0}
      # Network optimization
      - ENABLE_WIFI_BUFFER=true
      # Route planner option
      - USE_ROUTE_PLANNER=${USE_ROUTE_PLANNER:-false}
      # RViz option
      - USE_RVIZ=${USE_RVIZ:-false}
      # Unitree robot configuration
      - UNITREE_IP=${UNITREE_IP:-192.168.12.1}
      - UNITREE_CONN=${UNITREE_CONN:-LocalAP}

    # Volume mounts
    volumes:
      # X11 socket for GUI
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
      - ${HOME}/.Xauthority:/root/.Xauthority:rw
      # Mount Unity environment models (optional for hardware)
      - ./unity_models:/ros2_ws/src/ros-navigation-autonomy-stack/src/base_autonomy/vehicle_simulator/mesh/unity:rw
      # Mount entire dimos directory
      - ../..:/workspace/dimos:rw
      # Mount bagfiles directory
      - ./bagfiles:/ros2_ws/bagfiles:rw
      # Mount config files for easy editing
      - ./config:/ros2_ws/config:rw
      # Hardware-specific volumes
      - ./logs:/ros2_ws/logs:rw
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - /dev/bus/usb:/dev/bus/usb:rw
      - /sys:/sys:ro

    # Device access for hardware
    devices:
      # Joystick controller (specific device to avoid permission issues)
      - /dev/input/js0:/dev/input/js0
      # GPU access
      - /dev/dri:/dev/dri
      # Motor controller serial ports
      - ${MOTOR_SERIAL_DEVICE:-/dev/ttyACM0}:${MOTOR_SERIAL_DEVICE:-/dev/ttyACM0}
      # Additional serial ports (can be enabled via environment)
      # - /dev/ttyUSB0:/dev/ttyUSB0
      # - /dev/ttyUSB1:/dev/ttyUSB1
      # Cameras (can be enabled via environment)
      # - /dev/video0:/dev/video0

    # Working directory
    working_dir: /workspace/dimos

    # Command - launch the real robot system with foxglove_bridge
    command:
      - bash
      - -c
      - |
        echo "Checking joystick..."
        ls -la /dev/input/js0 2>/dev/null || echo "Warning: No joystick found at /dev/input/js0"
        cd /ros2_ws
        source install/setup.bash
        source /opt/dimos-venv/bin/activate
        if [ "$USE_ROUTE_PLANNER" = "true" ]; then
            echo "Starting real robot system WITH route planner..."
            ros2 launch vehicle_simulator system_real_robot_with_route_planner.launch.py &
        else
            echo "Starting real robot system (base autonomy)..."
            ros2 launch vehicle_simulator system_real_robot.launch.py &
        fi
        sleep 2
        if [ "$USE_RVIZ" = "true" ]; then
            echo "Starting RViz2..."
            if [ "$USE_ROUTE_PLANNER" = "true" ]; then
                ros2 run rviz2 rviz2 -d /ros2_ws/src/ros-navigation-autonomy-stack/src/route_planner/far_planner/rviz/default.rviz &
            else
                ros2 run rviz2 rviz2 -d /ros2_ws/src/ros-navigation-autonomy-stack/src/base_autonomy/vehicle_simulator/rviz/vehicle_simulator.rviz &
            fi
        fi
        # Launch Unitree control if ROBOT_CONFIG_PATH contains "unitree"
        if [[ "$ROBOT_CONFIG_PATH" == *"unitree"* ]]; then
            echo "Starting Unitree WebRTC control (IP: $UNITREE_IP, Method: $UNITREE_CONN)..."
            ros2 launch unitree_webrtc_ros unitree_control.launch.py robot_ip:=$UNITREE_IP connection_method:=$UNITREE_CONN &
        fi
        # Start twist relay for Foxglove Teleop (converts Twist -> TwistStamped)
        echo "Starting Twist relay for Foxglove Teleop..."
        python3 /usr/local/bin/twist_relay.py &
        # Start goal autonomy relay (publishes Joy to enable autonomy when goal_pose received)
        echo "Starting Goal Autonomy relay for Foxglove..."
        python3 /usr/local/bin/goal_autonomy_relay.py &
        echo "Starting Foxglove Bridge on port 8765..."
        echo "Connect via Foxglove Studio: ws://$(hostname -I | awk '{print $1}'):8765"
        ros2 launch foxglove_bridge foxglove_bridge_launch.xml port:=8765

    # Capabilities for hardware operations
    cap_add:
      - NET_ADMIN  # Network interface configuration
      - SYS_ADMIN  # System operations
      - SYS_TIME   # Time synchronization
